{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy, testX, testY):\n",
    "    \n",
    "    batch_size = 128\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    test_enc = to_categorical(testY)\n",
    "    model.fit(trainX, trainy_enc, batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "              validation_data=(testX, test_enc))\n",
    "    return model\n",
    "\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "    return result\n",
    "\n",
    "# # evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(members, normalized, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0448 - accuracy: 0.9849 - val_loss: 0.0182 - val_accuracy: 0.9936\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0160 - val_accuracy: 0.9945\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0139 - val_accuracy: 0.9953\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0146 - val_accuracy: 0.9952\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0146 - val_accuracy: 0.9952\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.0114 - val_accuracy: 0.9963\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0111 - val_accuracy: 0.9967\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0145 - val_accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0108 - val_accuracy: 0.9968\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.0105 - val_accuracy: 0.9969\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 0.0211 - val_accuracy: 0.9929\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0208 - accuracy: 0.9930 - val_loss: 0.0161 - val_accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.0139 - val_accuracy: 0.9953\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0127 - accuracy: 0.9958 - val_loss: 0.0152 - val_accuracy: 0.9952\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.0126 - val_accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0131 - val_accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0121 - val_accuracy: 0.9963\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0134 - val_accuracy: 0.9962\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0166 - val_accuracy: 0.9955\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0133 - val_accuracy: 0.9962\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0455 - accuracy: 0.9846 - val_loss: 0.0217 - val_accuracy: 0.9927\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.0144 - val_accuracy: 0.9951\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0131 - val_accuracy: 0.9956\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.0125 - val_accuracy: 0.9959\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0147 - val_accuracy: 0.9952\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0132 - val_accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.0130 - val_accuracy: 0.9962\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0125 - val_accuracy: 0.9961\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0129 - val_accuracy: 0.9963\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0135 - val_accuracy: 0.9963\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0449 - accuracy: 0.9845 - val_loss: 0.0216 - val_accuracy: 0.9925\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0207 - accuracy: 0.9931 - val_loss: 0.0178 - val_accuracy: 0.9939\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0130 - val_accuracy: 0.9955\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0133 - val_accuracy: 0.9956\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0141 - val_accuracy: 0.9958\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0150 - val_accuracy: 0.9955\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0140 - val_accuracy: 0.9961\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.0137 - val_accuracy: 0.9962\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0147 - val_accuracy: 0.9961\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.0188 - val_accuracy: 0.9938\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0201 - accuracy: 0.9933 - val_loss: 0.0153 - val_accuracy: 0.9947\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.0139 - val_accuracy: 0.9954\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0140 - val_accuracy: 0.9953\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0123 - val_accuracy: 0.9959\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0133 - val_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0111 - val_accuracy: 0.9963\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0134 - val_accuracy: 0.9959\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.0135 - val_accuracy: 0.9962\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0061 - accuracy: 0.9979 - val_loss: 0.0120 - val_accuracy: 0.9965\n"
     ]
    }
   ],
   "source": [
    "n_members = 5 #no of stacked models\n",
    "members = [\n",
    "    fit_model(X_train, Y_train, X_test, Y_test) for _ in range(n_members)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.997\n",
      "Model 2: 0.996\n",
      "Model 3: 0.996\n",
      "Model 4: 0.996\n",
      "Model 5: 0.997\n",
      "Equal Weights Score: 0.987\n",
      "Optimized Weights: [0.26677845 0.0296783  0.3296601  0.13227702 0.24160613]\n",
      "Optimized Weights Score: 0.988\n"
     ]
    }
   ],
   "source": [
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(Y_test)\n",
    "for i in range(n_members):\n",
    "    _, test_acc = members[i].evaluate(X_test, testy_enc, verbose=0)\n",
    "    print('Model %d: %.3f' % (i+1, test_acc))\n",
    "\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, X_test, Y_test)\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n",
    "\n",
    "# arguments to the loss function\n",
    "search_arg = (members, X_test, Y_test)\n",
    "\n",
    "# global optimization of ensemble weights\n",
    "\"\"\"\n",
    "The differential_evolution() SciPy function requires that function is specified to evaluate a set of weights \n",
    "and return a score to be minimized.  We can minimize the classification error (1 – accuracy).\n",
    "\"\"\"\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=10, tol=1e-7)\n",
    "\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights(weights obtained from optimization)\n",
    "score = evaluate_ensemble(members, weights, X_test, Y_test)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted ensemble probability and uncertainity prediction\n",
    "import math\n",
    "import numpy as np\n",
    "def ensemble_predictions_prob(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "    \n",
    "    #weighted var of ensemble members\n",
    "    variance = tensordot((yhats-summed)**2, weights, axes=((0),(0)))\n",
    "    return (summed, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: 7, predicted: 7, confidence: 0.9999995306633523, var: 1.9661382135270723e-13\n",
      "actual: 2, predicted: 2, confidence: 0.9999999999999999, var: 1.2325951644078308e-32\n",
      "actual: 1, predicted: 1, confidence: 0.9999991824032886, var: 1.2048720576961065e-12\n",
      "actual: 0, predicted: 0, confidence: 0.9999059356697367, var: 3.400109780481828e-08\n",
      "actual: 4, predicted: 4, confidence: 0.9818585128565763, var: 0.0006679036189163716\n",
      "actual: 1, predicted: 1, confidence: 0.9999999681975303, var: 2.779752723714715e-15\n",
      "actual: 4, predicted: 4, confidence: 0.9976635675615587, var: 9.581343838659796e-06\n",
      "actual: 9, predicted: 9, confidence: 0.9992633131759838, var: 7.20725545011689e-07\n",
      "actual: 5, predicted: 5, confidence: 0.8096238211252196, var: 0.09624524929470496\n",
      "actual: 9, predicted: 9, confidence: 0.999982540330106, var: 8.214497314532808e-10\n"
     ]
    }
   ],
   "source": [
    "ensemble_prob, var = ensemble_predictions_prob(members, weights, X_test)\n",
    "\n",
    "for it, prob, vari in zip(Y_test[:10], ensemble_prob[:10], var[:10]):\n",
    "    print('actual: '+str(it)+', predicted: '+str(np.argmax(prob))+', confidence: '+str(prob[np.argmax(prob)])+', var: '+str(vari[np.argmax(prob)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
